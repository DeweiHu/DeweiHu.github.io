## [Hyperparameter Sensitivity] Hyperparameter Sensitivuty in Deep Outlier Detection (Analysis and a Scalable Hyper-Ensemble Solution) 
- [x] Public accessibility to [paper](https://arxiv.org/pdf/2206.07647.pdf)
- [x] Reproducibility [code](https://github.com/xyvivian/ROBOD)

### Summary
This work is presented in two folds:

> * Experimtally show that existing models proposed for classification task (e.g. MNIST and CIFAR 10) are sensitive to the setting of hyperparameters. 

The following table shows that the author leverage a grid search on several of the hyperparameters of the model and train on each of them. This will resultant in 
a huge number of combinations. The testing results of these models are illustrated in the boxplot. Obviously, different setting of hyperparameters can cause huge
variation of performance. Or in other words, <ins>the value of judging a model in terms of performance on a suggested set of hyperparameter is suspicious.</ins> 
A cherry-picked set of hyperparameters can have a greater impact on the model than the architecture itself. And this experiment is sufficient to prove that most 
deep models, if not all, are sensitive to hyperparamters. 
<p align="center">
  <img src="/assets/221116_fold1table.png" alt="drawing" width="500"/>
</p>
<p align="center">
  <img src="/assets/221116_fold1boxplot.png" alt="drawing" width="700"/>
</p>
Given this conclusion, the auther introduce a method to eliminate this sensitivity so that the deep model can be robust with regard to the hyperparamters.


> * Propose an hyperparameter ensembling method to improve the robustness as well as the general performance.

<p align="center">
  <img src="/assets/221116_fold2img.png" alt="drawing" width="700"/>
</p>

The general ensemble method is simply take the mean of prediction of models with different configurations (defined as **Hyper-ensemble**). The design of ROBOD is
shown in the image above. There are two main elements of the method:
1. A multi-layer auto-encoder with skip connection (AE-S): This is very similar with U-Net, but the author utilize it with a novel intuition. It is regarded as a stack
of different AEs with diverse depth. For example, AE-2 is the one with depth of 2. In this way, the hyperparamter of depth is included in a single model.
2. A zero-masking strategy that represent the variation in terms of width as it is depicted in the image. Note that the weight W is shared between models.

